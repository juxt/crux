[#getting-started-example]
== Getting Started

This guide contains simple steps transacting some data and running a
simple query.

== Preconceptions you might want to know

- *Datalog* and *EDN*. Crux supports an EDN-flavoured version of Datalog.
  You can find some prolific training http://www.learndatalogtoday.org/chapter/1[here].

- Concept of database temporality and consequent bitemporality.

- *Clojure* succinct and pragmatic data-oriented language with strong support
  for immutability and paralellism. https://clojure.org/[More].



== Setting things up

First setup Crux. If you want to play about, it's advised you run Crux
in a <<setup#standalone,standalone configuration>> first.

=== Write a transaction:

[source,clj]
----
include::./examples.clj[tags=submit-tx]
----
Id for Crux is likely to be unnecessary to restate in future, or entirely optional, TBD.

=== Make a query:

[source,clj]
----
include::./examples.clj[tags=query]
----

You should get:

[source,clj]
----
include::./examples.clj[tags=should-get]
----

An entity query would be:
[source,clj]
----
include::./examples.clj[tags=query-entity]
----
You should get:

[source,clj]
----
include::./examples.clj[tags=should-get-entity]
----


== Query

Crux query syntax is that of Datalog using Clojure's EDN, inspired by
Datomic (later adopted by other DBs such as DataScript).

=== Basic Query

[source,clj]
----
include::./examples.clj[tags=query]
----

The `db` is retrieved via a call to `crux.api/db`.




=== Time Queries

`crux.api/db` optionally takes valid time and transaction time as
additional arguments. In the below example we query using valid time
only.

[source,clj]
----
include::./examples.clj[tags=query-valid-time]
----

In the next example we query using both valid time and transaction
time temporal coordinates. This call will block until the local index
has seen the transaction time

[source,clj]
----
include::./examples.clj[tags=query-tx-time]
----

NOTE: TODO: The above doesn't yeild a result

=== Lazy Queries

NOTE: TODO Make an example

The `crux.api/q` takes 2 or 3 arguments, `db` and `q` but also
optionally a `snapshot` which is already opened and managed by the
caller (using `with-open` for example). This version of the call
returns a lazy sequence of the results, while the other version
provides a set. A snapshot can be retreived from a `kv` instance via
`crux.kv-store/new-snapshot`.

=== Rules

Crux query capability is easiest summarized via an example:

[source,clj]
----
(q/q db
    '{:find  [?e2]
      :where [(follow ?e1 ?e2)]
      :args [{:?e1 :1}]
      :rules [[(follow ?e1 ?e2)
              [?e1 :follow ?e2]]
             [(follow ?e1 ?e2)
              [?e1 :follow ?t]
              (follow ?t ?e2)]]})
----

The `:args` key contains a relation where each map is expected to have
the same keys. These keys are turned into logic variable symbols and the
relation is joined with the rest of the query. The elements must
implement `Comparable`.

Crux does not support variables in the attribute position. The entity
position is hard coded to mean the `:crux.db/id` field.

== Transactions

The four transaction (write) operations are as follows:

[source,clojure]
----
[:crux.tx/put :http://dbpedia.org/resource/Pablo_Picasso
"090622a35d4b579d2fcfebf823821298711d3867"
#inst "2018-05-18T09:20:27.966-00:00"]

[:crux.tx/cas :http://dbpedia.org/resource/Pablo_Picasso
"090622a35d4b579d2fcfebf823821298711d3867"
"048ebba27e1da223ce97dded59d46e069ddf921b"
#inst "2018-05-18T09:21:31.846-00:00"]

[:crux.tx/delete :http://dbpedia.org/resource/Pablo_Picasso
#inst "2018-05-18T09:21:52.151-00:00"]

[:crux.tx/evict :http://dbpedia.org/resource/Pablo_Picasso
#inst "2018-05-18T09:21:52.151-00:00"]
----

The valid time is optional and defaults to transaction time, which is
taken from the Kafka log. Crux currently writes into the past at a
single point, so to overwrite several versions or a range in time, one
is required to submit a transaction containing several operations.
Eviction works a bit differently, and all versions at or before the
provided valid time are evicted.

The hashes are the SHA-1 content hash of the documents. Crux uses an
attribute `:crux.db/id` on the documents that has to line up with the id
it is submitted under.

A document looks like this:

[source,clj]
----
{:crux.db/id :http://dbpedia.org/resource/Pablo_Picasso
 :name "Pablo"
 :last-name "Picasso"}
----

In practice when using Crux, one calls `crux.db/submit-tx` with a set of
transaction operations as above, where the hashes are replaced with
actual documents:

[source,clj]
----
[[:crux.tx/put :http://dbpedia.org/resource/Pablo_Picasso
 {:crux.db/id :http://dbpedia.org/resource/Pablo_Picasso
  :name "Pablo"
  :last-name "Picasso"}
 #inst "2018-05-18T09:20:27.966-00:00"]]
----

For each operation the id and the document are hashed, and this version
is submitted to the `tx-topic` in Kafka. The document itself is
submitted to the `doc-topic`, using its content hash as key. This latter
topic is compacted, which enables later deletion of documents.

If the transaction contains CAS operations, all CAS operations must pass
their pre-condition check or the entire transaction is aborted. This
happens at the query node during indexing, and not when submitting the
transaction.

== API

Please consult the JavaDocs for the official Crux API.

https://juxt.pro/crux/docs/javadoc/index.html[API JavaDoc]

[#rest]
== REST API

Crux offers a small REST API that allows one to use Crux in a
conventional SaaS way, deploying Kafka and query nodes into AWS and
interacting with Crux over HTTP. This mode does not support all
features.

When using the REST API, the user doesnâ€™t interact directly with
Kafka, but calls one of the query nodes (potentially behind a load
balancer) over HTTP to interact with Crux. As the query nodes might be
at different points in the index, and different queries might go to
differnet nodes, this may cause read consistency issues.

The REST API also provides an experimental endpoint for SPARQL 1.1
Protocol queries under `/sparql/`, rewriting the query into the Crux
datalog dialect. Only a small subset of SPARQL is supported, and no
other RDF features are available.

.API
[#table-conversion%header,cols="d,d,d"]
|===
|uri|method|description
|<<#home,`/`>>|GET|returns various details about the state of the database
|<<#document, `/document/[content-hash]`>>|GET or POST|returns the document for a given hash
|<<#entity, `/entity`>>|POST|Returns an entity for a given ID and optional valid-time/transaction-time co-ordinates
|<<#entity-tx, `/entity-tx`>>|POST|Returns the `:put` or `:cas` transaction that most recently set a key
|<<#history, `/history/[:key]`>>|GET OR POST|Returns the transaction history of a key
|<<#query, `/query`>>|POST|Takes a datalog query and returns its results
|<<#query-stream, `/query-stream`>>|POST| Same as `/query` but the results are streamed
|<<#sync, `/sync`>>|GET| Wait until the Kafka consumer's lag is back to 0
|<<#tx-log, `/tx-log`>>|GET| Returns a list of all transactions
|<<#tx-log-post, `/tx-log`>>|POST|The "write" endpoint, to post transactions.
|===

[#home]
=== GET `/`

Returns various details about the state of the database. Can be used as a health check.

[source,bash]
----
curl -X GET $nodeURL/
----
[source,clj]
----
{:crux.kv/kv-backend "crux.kv.rocksdb.RocksKv",
 :crux.kv/estimate-num-keys 92,
 :crux.kv/size 72448,
 :crux.zk/zk-active? true,
 :crux.tx-log/consumer-state
   {:crux.kafka.topic-partition/crux-docs-0
      {:offset 25,
       :time #inst "2019-01-08T11:06:41.867-00:00",
       :lag 0},
    :crux.kafka.topic-partition/crux-transaction-log-0
      {:offset 19,
       :time #inst "2019-01-08T11:06:41.869-00:00",
       :lag 0}}}
----

NOTE: `estimate-num-keys` is an (over)estimate of the number of transactions in the log (each of which is a key in RocksDB). RocksDB https://github.com/facebook/rocksdb/wiki/RocksDB-FAQ[does not provide] an exact key count.

[#document]
=== GET/POST `/document/[content-hash]`

Returns the document stored under that hash, if it exists.

[source,bash]
----
curl -X GET $nodeURL/document/7af0444315845ab3efdfbdfa516e68952c1486f2
----
[source,clj]
----
{:crux.db/id :foobar, :name "FooBar"}
----
NOTE: Hashes for older versions of a document can be obtained with `/history`, under the `:crux.db/content-hash` keys.

[#entity]
=== POST `/entity`

Takes a key and, optionally, a `:valid-time` and/or `:transact-time` (defaulting to now). Returns the value stored under that key at those times.

See <<bitemp.adoc#,bitemporality explained>> for more information.

[source,bash]
----
curl -X POST \
     -H "Content-Type: application/edn" \
     -d '{:eid :tommy}' \
     $nodeURL/entity
----

[source,clj]
----
{:crux.db/id :tommy, :name "Tommy", :last-name "Petrov"}
----

[source,bash]
----
curl -X POST \
     -H "Content-Type: application/edn" \
     -d '{:eid :tommy :valid-time #inst "1999-01-08T14:03:27.254-00:00"}' \
     $nodeURL/entity
----

[source,clj]
----
nil
----

[#entity-tx]
=== POST `/entity-tx`

Takes a key and, optionally, `:valid-time` and/or `:transact-time` (defaulting to now). Returns the `:put` or `:cas` transaction that most recently set that key at those times.

See <<bitemp.adoc, #bitemporality explained>> for more information.

[source,bash]
----
curl -X POST \
     -H "Content-Type: application/edn" \
     -d '{:eid :foobar}' \
     $nodeURL/entity-tx
----
[source,clj]
----
{:crux.db/id "8843d7f92416211de9ebb963ff4ce28125932878",
 :crux.db/content-hash "7af0444315845ab3efdfbdfa516e68952c1486f2",
 :crux.db/valid-time #inst "2019-01-08T16:34:47.738-00:00",
 :crux.tx/tx-id 0,
 :crux.tx/tx-time #inst "2019-01-08T16:34:47.738-00:00"}
----

[#history]
=== GET/POST `/history/[:key]`

Returns the transaction history of a key, from newest to oldest transaction time.

[source,bash]
----
curl -X GET $nodeURL/history/:ivan
----

[source,clj]
----
[{:crux.db/id "a15f8b81a160b4eebe5c84e9e3b65c87b9b2f18e",
  :crux.db/content-hash "c28f6d258397651106b7cb24bb0d3be234dc8bd1",
  :crux.db/valid-time #inst "2019-01-07T14:57:08.462-00:00",
  :crux.tx/tx-id 14,
  :crux.tx/tx-time #inst "2019-01-07T16:51:55.185-00:00"}

 {...}]
----

[#query]
=== POST `/query`

Takes a datalog query and returns its results.

[source,bash]
----
curl -X POST \
     -H "Content-Type: application/edn" \
     -d '{:query {:find [e] :where [[e :last-name "Petrov"]]}}' \
     $nodeURL/query
----

[source,clj]
----
#{[:boris][:ivan]}
----

[#query-stream]
=== POST `/query-stream`

Same as `/query` but the results are streamed.

[#sync]
=== GET `/sync`

Wait until the Kafka consumer's lag is back to 0 (i.e. when it no longer has pending transactions to write). Timeout is 10 seconds by default, but can be specified as a parameter in miliseconds. Returns the transaction time of the most recent transaction.

[source,bash]
----
curl -X GET $nodeURL/sync?timeout=500
----

[source,clj]
----
#inst "2019-01-08T11:06:41.869-00:00"
----

[#tx-log]
=== GET `/tx-log`

Returns a list of all transactions, from oldest to newest transaction time.

[source,bash]
----
curl -X GET $nodeURL/tx-log
----

[source,clj]
----
({:crux.tx/tx-time #inst "2019-01-07T15:11:13.411-00:00",
  :crux.tx/tx-ops [[
    :crux.tx/put "a15f8b81a160b4eebe5c84e9e3b65c87b9b2f18e" "c28f6d258397651106b7cb24bb0d3be234dc8bd1"
    #inst "2019-01-07T14:57:08.462-00:00"]],
  :crux.tx/tx-id 0}

 {:crux.tx/tx-time #inst "2019-01-07T15:11:32.284-00:00",
  ...})
----

[#tx-log-post]
=== POST `/tx-log`

Takes a vector of transactions (any combination of `:put`, `:delete`, `:cas` and `:evict`) and executes them in order. This is the only "write" endpoint.

[source,bash]
----
curl -X POST \
     -H "Content-Type: application/edn" \
     -d '[[:crux.tx/put :ivan {:crux.db/id :ivan, :name "Ivan" :last-name "Petrov"}],
          [:crux.tx/put :boris {:crux.db/id :boris, :name "Boris" :last-name "Petrov"}],
          [:crux.tx/delete :maria  #inst "2012-05-07T14:57:08.462-00:00"]]' \
     $nodeURL/tx-log
----
[source,clj]
----
{:crux.tx/tx-id 7, :crux.tx/tx-time #inst "2019-01-07T16:14:19.675-00:00"}
----
